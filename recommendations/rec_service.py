import logging
import pathlib
import pickle
import shutil
import asyncio
import grpc
from typing import Tuple, Optional, Dict, List, Any
from threading import RLock
import datetime
import time
import copy
import numpy as np

import pandas as pd
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
import pytz

import warnings
from lightfm import LightFM
from pandas import DataFrame
from rectools import Columns
from rectools.dataset import Dataset, Interactions
from rectools.model_selection import TimeRangeSplitter
from rectools.models import LightFMWrapperModel, load_model
from rectools.models.base import ModelBase

from recommendations.data_preparer import DataPrepareService, BlacklistManager
from recommendations.model_registry import ModelRegistry
from recommendations.cache_service import RecommendationCache
from recommendations.auth import auth_required

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
from recommendations.config import Config

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)


class ModelManager:
    _instance = None
    _model: Optional[ModelBase] = None
    _dataset: Optional[Dataset] = None
    _version = 0
    _rw_lock = RLock()
    _file_lock = asyncio.Lock()

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, dp_service: DataPrepareService, model_config=None) -> None:
        self.dp_service = dp_service
        self.logger = logging.getLogger(__name__)
        self.user_embeddings_ = None
        self.item_embeddings_ = None
        if model_config is None:
            config_dict = Config.MODEL_PARAMS['default']
        else:
            config_dict = model_config
        self.config = config_dict
        self.num_threads = config_dict.get("num_threads", 4)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –∏–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ–Ω–µ–¥–∂–µ—Ä –º–æ–¥–µ–ª–µ–π, –∑–∞–≥—Ä—É–∂–∞—è –∞–∫—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ —Ä–µ–µ—Å—Ç—Ä–∞"""
        registry = ModelRegistry()
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–∫—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏
        active_model = registry.get_active_model()
        
        if active_model and active_model.get('file_path') and active_model.get('dataset_path'):
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ –¥–∞—Ç–∞—Å–µ—Ç
                self._load_from_registry(active_model)
                
                if not self._model.is_fitted:
                    logger.warning("–ó–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞! –ù–∞—á–∏–Ω–∞–µ–º –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ...")
                    await self.train()
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}. –ó–∞–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ...")
                await self.train()
        else:
            logger.warning("–ê–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ä–µ–µ—Å—Ç—Ä–µ. –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
            await self.train()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
        cache = RecommendationCache()
        try:
            await cache.initialize()
            logger.info("–°–µ—Ä–≤–∏—Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–µ—Ä–≤–∏—Å–∞ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")

    def _load_from_registry(self, model_info: Dict[str, Any]):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —Ä–µ–µ—Å—Ç—Ä–∞ –º–æ–¥–µ–ª–µ–π"""
        with self._rw_lock:
            try:
                model_path = model_info.get('file_path')
                dataset_path = model_info.get('dataset_path')
                
                with open(dataset_path, 'rb') as f:
                    self._dataset = pickle.load(f)
                self._model = load_model(f=model_path)
                
                if not self._model.is_fitted:
                    raise ValueError("–ó–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞.")
                    
                self._version = model_info.get('version', 0)
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å –≤–µ—Ä—Å–∏–∏ {self._version}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
                raise
    
    @auth_required("fit_partial")
    async def fit_partial(self, new_interactions: DataFrame = None, new_user_features: DataFrame = None, context = None):
        """
        –ß–∞—Å—Ç–∏—á–Ω–æ –¥–æ–æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–µ –ø–µ—Ä–µ–æ–±—É—á–∞—è –Ω–∞ –≤—Å–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ.
        
        Args:
            new_interactions: DataFrame —Å –Ω–æ–≤—ã–º–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
            new_user_features: DataFrame —Å –Ω–æ–≤—ã–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç gRPC –∑–∞–ø—Ä–æ—Å–∞
        """
        model, current_dataset = await self.get_model()
        if not model.is_fitted:
            logger.warning("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å —á–∞—Å—Ç–∏—á–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ.")
            return False
        
        try:
            logger.info("–ù–∞—á–∏–Ω–∞–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø—É—Å—Ç—ã–µ
            if new_interactions is None or new_interactions.empty:
                logger.warning("–ù–µ—Ç –Ω–æ–≤—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
                return False
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –¥–∞—Ç–∞—Å–µ—Ç
            updated_dataset = current_dataset.clone()
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
            if new_interactions is not None and not new_interactions.empty:
                interactions_to_add = Interactions(new_interactions)
                updated_dataset.update_interactions(interactions_to_add)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –µ—Å–ª–∏ –µ—Å—Ç—å
            if new_user_features is not None and not new_user_features.empty:
                updated_dataset.update_user_features(new_user_features)
            
            # –ß–∞—Å—Ç–∏—á–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ
            model.fit_partial(updated_dataset, epochs=3)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
            model_id = await self.register_model(model, updated_dataset, {
                'model_type': 'lightfm',
                'training_type': 'partial',
                'epochs': 3,
                'interactions_count': len(new_interactions)
            })
            
            # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫–µ—à
            cache = RecommendationCache()
            await cache.invalidate_all_recommendations()
            
            logger.info(f"–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –¥–æ–æ–±—É—á–µ–Ω–∞ –∏ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ —Å ID: {model_id}")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {str(e)}")
            raise

    @auth_required("train")
    async def train(self, parameters: Optional[Dict[str, Any]] = None, context = None):
        """
        –û–±—É—á–∞–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
        
        Args:
            parameters: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç gRPC –∑–∞–ø—Ä–æ—Å–∞
        
        Returns:
            ID –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–µ—Å—Ç—Ä–µ
        """
        try:
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            params = {
                'model_type': 'lightfm',
                'no_components': 100,
                'loss': 'bpr',
                'random_state': 60,
                'num_threads': self.num_threads,
                'epochs': self.config["epochs"],
                'item_alpha': 0.0,
                'user_alpha': 0.0
            }
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –µ—Å–ª–∏ –æ–Ω–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã
            if parameters:
                params.update(parameters)
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            await BlacklistManager.refresh_blacklist()
            user_features = await self.dp_service.get_users_features()
            items_features = await self.dp_service.get_titles_features()
            interactions = await self.dp_service.get_interactions()
            
            if interactions.empty:
                raise ValueError("–î–∞–Ω–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –ø—É—Å—Ç—ã. –û–±—É—á–µ–Ω–∏–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ.")

            # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            dataset = Dataset.construct(
                interactions_df=interactions,
                user_features_df=user_features,
                cat_user_features=["age_group", "sex", "preference"],
                item_features_df=items_features,
                cat_item_features=["type_id", "genres", "categories", "count_chapters", "age_limit", "relation_list"],
            )

            # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            model = LightFMWrapperModel(
                LightFM(
                    no_components=params['no_components'],
                    loss=params['loss'],
                    random_state=params['random_state'],
                    item_alpha=params['item_alpha'],
                    user_alpha=params['user_alpha']
                ),
                num_threads=params['num_threads'],
                epochs=params['epochs']
            )

            logger.info("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
            start_time = time.time()
            model.fit(dataset)
            
            if not model.is_fitted:
                raise ValueError("–û—à–∏–±–∫–∞! –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∏–ª–∞—Å—å!")

            # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–µ—Å—Ç—Ä–µ
            model_id = await self.register_model(model, dataset, params)
            
            # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫–µ—à —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            cache = RecommendationCache()
            await cache.invalidate_all_recommendations()
            
            logger.info(f"–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ —Å ID: {model_id}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
            self.user_embeddings_ = model.get_user_embeddings()
            self.item_embeddings_ = model.get_item_embeddings()
            
            return model_id

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {str(e)}")
            raise

    async def register_model(self, model: ModelBase, dataset: Dataset, parameters: Dict[str, Any]) -> str:
        """
        –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –≤ —Ä–µ–µ—Å—Ç—Ä–µ –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –µ–µ –∫–∞–∫ –∞–∫—Ç–∏–≤–Ω—É—é.
        
        Args:
            model: –û–±—ä–µ–∫—Ç –º–æ–¥–µ–ª–∏
            dataset: –û–±—ä–µ–∫—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞
            parameters: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
            
        Returns:
            ID –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–µ—Å—Ç—Ä–µ
        """
        registry = ModelRegistry()
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–µ—Å—Ç—Ä–µ
        model_id = registry.register_model(
            name="lightfm_model",
            parameters=parameters
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏
        model_path, dataset_path = registry.save_model_files(model_id, model, dataset)
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –∫–∞–∫ –∞–∫—Ç–∏–≤–Ω—É—é
        registry.set_active_model(model_id)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç–∏
        with self._rw_lock:
            self._model = model
            self._dataset = dataset
            self._version = parameters.get('version', self._version + 1)
        
        return model_id

    async def get_model(self) -> Tuple[ModelBase, Dataset]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â—É—é –∞–∫—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –∏ –¥–∞—Ç–∞—Å–µ—Ç.
        
        Returns:
            –ö–æ—Ä—Ç–µ–∂ —Å –º–æ–¥–µ–ª—å—é –∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–º
        """
        if self._model is None or self._dataset is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        return self._model, self._dataset
    
    @auth_required("list_models")
    async def list_models(self, limit: int = 20, offset: int = 0, context = None) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –∏–∑ —Ä–µ–µ—Å—Ç—Ä–∞.
        
        Args:
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π
            offset: –°–º–µ—â–µ–Ω–∏–µ –¥–ª—è –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç gRPC –∑–∞–ø—Ä–æ—Å–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –º–æ–¥–µ–ª—è—Ö
        """
        registry = ModelRegistry()
        return registry.list_models(limit=limit, offset=offset)
    
    @auth_required("get_model_info")
    async def get_model_info(self, model_id: str, context = None) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏ –∏–∑ —Ä–µ–µ—Å—Ç—Ä–∞.
        
        Args:
            model_id: ID –º–æ–¥–µ–ª–∏
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç gRPC –∑–∞–ø—Ä–æ—Å–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –º–æ–¥–µ–ª–∏
        """
        registry = ModelRegistry()
        return registry.get_model_info(model_id)
    
    @auth_required("set_active_model")
    async def set_active_model(self, model_id: str, context = None) -> bool:
        """
        –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –∫–∞–∫ –∞–∫—Ç–∏–≤–Ω—É—é.
        
        Args:
            model_id: ID –º–æ–¥–µ–ª–∏
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç gRPC –∑–∞–ø—Ä–æ—Å–∞
            
        Returns:
            True, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∫–∞–∫ –∞–∫—Ç–∏–≤–Ω–∞—è
        """
        registry = ModelRegistry()
        model_info = registry.get_model_info(model_id)
        
        if not model_info:
            return False
            
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –∫–∞–∫ –∞–∫—Ç–∏–≤–Ω—É—é –≤ —Ä–µ–µ—Å—Ç—Ä–µ
        if not registry.set_active_model(model_id):
            return False
            
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç—å
        try:
            self._load_from_registry(model_info)
            
            # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫–µ—à —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            cache = RecommendationCache()
            await cache.invalidate_all_recommendations()
            
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∞–∫—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    @auth_required("schedule_training")
    async def schedule_training(self, parameters: Optional[Dict[str, Any]] = None, scheduled_at: Optional[datetime.datetime] = None, context = None) -> str:
        """
        –ü–ª–∞–Ω–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏.
        
        Args:
            parameters: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
            scheduled_at: –í—Ä–µ–º—è –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è)
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç gRPC –∑–∞–ø—Ä–æ—Å–∞
            
        Returns:
            ID –∑–∞–¥–∞–Ω–∏—è –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ
        """
        registry = ModelRegistry()
        
        if scheduled_at is None:
            scheduled_at = datetime.datetime.now()
            
        # –ü–ª–∞–Ω–∏—Ä—É–µ–º –∑–∞–¥–∞–Ω–∏–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ
        job_id = registry.schedule_training_job(
            parameters=parameters or {},
            scheduled_at=scheduled_at
        )
        
        return job_id
    
    @property
    def current_version(self) -> int:
        return self._version

    def _get_model(self):
        model_config = model_config_class(**self.config["model"])
        model = model_cls(config=model_config)
        return model

    def train(self, train_user_idx, train_item_idx, train_weights):
        model = self._get_model()
        self.logger.info('Training model...')
        start_time = time.time()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä threads –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        model.fit(
            interactions=(train_user_idx, train_item_idx, train_weights),
            num_threads=self.num_threads,
            epochs=self.config["epochs"]
        )
        
        self.logger.info(f'Model training completed in {time.time() - start_time:.2f} seconds')
        self.model = model
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        self.user_embeddings_ = model.get_user_embeddings()
        self.item_embeddings_ = model.get_item_embeddings()
        
        return model

    def train_with_validation(self, train_user_idx, train_item_idx, train_weights, 
                            val_user_idx, val_item_idx, val_weights, k=10):
        model = self._get_model()
        self.logger.info('Training model with validation...')
        
        best_ndcg = 0
        best_epoch = 0
        best_model = None
        
        epochs = self.config["epochs"]
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å —Å —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π
        for epoch in range(1, epochs + 1):
            start_time = time.time()
            
            # –û–±—É—á–∞–µ–º –æ–¥–Ω—É —ç–ø–æ—Ö—É
            model.fit(
                interactions=(train_user_idx, train_item_idx, train_weights),
                num_threads=self.num_threads,
                epochs=1
            )
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            val_ndcg = self._evaluate_ndcg(model, val_user_idx, val_item_idx, val_weights, k=k)
            
            epoch_time = time.time() - start_time
            self.logger.info(f'Epoch {epoch}/{epochs}, NDCG@{k}: {val_ndcg:.4f}, Time: {epoch_time:.2f}s')
            
            if val_ndcg > best_ndcg:
                best_ndcg = val_ndcg
                best_epoch = epoch
                best_model = copy.deepcopy(model)
                self.logger.info(f'New best model at epoch {epoch} with NDCG@{k}: {val_ndcg:.4f}')
            
            # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞, –µ—Å–ª–∏ –Ω–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è –≤ —Ç–µ—á–µ–Ω–∏–µ 3 —ç–ø–æ—Ö
            if epoch - best_epoch >= 3:
                self.logger.info(f'Early stopping at epoch {epoch}, best epoch: {best_epoch}')
                break
        
        self.model = best_model if best_model else model
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        self.user_embeddings_ = self.model.get_user_embeddings()
        self.item_embeddings_ = self.model.get_item_embeddings()
        
        return self.model

    def _evaluate_ndcg(self, model, user_idx, item_idx, weights, k=10):
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç NDCG@k –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö"""
        from rectools.metrics import calc_ndcg
        
        # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        unique_users = np.unique(user_idx)
        
        # –°–æ–±–∏—Ä–∞–µ–º –∏—Å—Ç–∏–Ω–Ω—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        true_relevance = {}
        for u, i, w in zip(user_idx, item_idx, weights):
            if u not in true_relevance:
                true_relevance[u] = {}
            true_relevance[u][i] = w
        
        # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        all_items = np.unique(item_idx)
        predictions = []
        
        for user in unique_users:
            user_preds = model.predict(user, all_items, num_threads=self.num_threads)
            top_items_idx = np.argsort(-user_preds)[:k]
            top_items = all_items[top_items_idx]
            
            user_relevance = [true_relevance.get(user, {}).get(item, 0) for item in top_items]
            predictions.append((user, top_items, user_relevance))
        
        # –í—ã—á–∏—Å–ª—è–µ–º NDCG
        ndcg_sum = 0
        for user, items, relevance in predictions:
            ndcg_sum += calc_ndcg(relevance, k)
        
        return ndcg_sum / len(unique_users)

    def predict(self, users, items=None, k=None, filtered_items=None):
        if not hasattr(self, 'model'):
            raise ValueError("Model is not trained yet.")
        
        # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        if items is None:
            if filtered_items is not None:
                items = filtered_items
            else:
                items = np.arange(self.model.get_item_embeddings().shape[0])
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        scores = self.model.predict(users, items, num_threads=self.num_threads)
        
        if k is not None:
            # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞—Ö–æ–¥–∏–º —Ç–æ–ø-k —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            if len(users) == 1:
                # –ï—Å–ª–∏ –æ–¥–∏–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å, –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-k —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                top_indices = np.argsort(-scores)[:k]
                return items[top_indices], scores[top_indices]
            else:
                # –ï—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –Ω–∞—Ö–æ–¥–∏–º —Ç–æ–ø-k –¥–ª—è –∫–∞–∂–¥–æ–≥–æ
                result_items = []
                result_scores = []
                
                for i, user in enumerate(users):
                    user_scores = scores[i]
                    top_indices = np.argsort(-user_scores)[:k]
                    result_items.append(items[top_indices])
                    result_scores.append(user_scores[top_indices])
                
                return result_items, result_scores
        
        return items, scores

    def get_similar_items(self, item_ids, k=10):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç k –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –≤ item_ids
        –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        if self.item_embeddings_ is None:
            self.item_embeddings_ = self.model.get_item_embeddings()
        
        n_items = self.item_embeddings_.shape[0]
        
        result = []
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        for item_id in item_ids:
            if item_id >= n_items:
                self.logger.warning(f"Item ID {item_id} is out of range. Skipping.")
                result.append(([], []))
                continue
            
            item_vector = self.item_embeddings_[item_id]
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å–æ –≤—Å–µ–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏
            dot_products = np.dot(self.item_embeddings_, item_vector)
            item_norm = np.linalg.norm(item_vector)
            all_norms = np.linalg.norm(self.item_embeddings_, axis=1)
            
            # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
            similarities = np.zeros_like(dot_products)
            nonzero_indices = all_norms > 0
            similarities[nonzero_indices] = dot_products[nonzero_indices] / (all_norms[nonzero_indices] * item_norm)
            
            # –ò—Å–∫–ª—é—á–∞–µ–º —Å–∞–º —ç–ª–µ–º–µ–Ω—Ç
            similarities[item_id] = -1
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-k –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            top_indices = np.argsort(-similarities)[:k]
            top_similarities = similarities[top_indices]
            
            result.append((top_indices, top_similarities))
        
        return result


class RecService:
    _scheduler: Optional[AsyncIOScheduler] = None

    @classmethod
    def start_scheduler(cls):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞–¥–∞–Ω–∏–π"""
        if cls._scheduler is None:
            cls._scheduler = AsyncIOScheduler(timezone=pytz.timezone("Europe/Moscow"))
            
            # –ü–ª–∞–Ω–∏—Ä—É–µ–º –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ 3:00
            cls._scheduler.add_job(
                cls._scheduled_train,
                trigger=CronTrigger(hour=3, minute=0),
                max_instances=1
            )
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞–Ω–∏–π –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç
            cls._scheduler.add_job(
                cls._check_scheduled_jobs,
                trigger=CronTrigger(minute='*/5'),
                max_instances=1
            )
            
            cls._scheduler.start()
            logger.info("‚úÖ Scheduler started")

    @staticmethod
    async def _handle_request(context: grpc.ServicerContext):
        try:
            if ModelManager().model is None:
                await context.abort(grpc.StatusCode.FAILED_PRECONDITION, "‚ùå Model not initialized")
        except Exception as e:
            logger.error(f"üî• Internal error: {str(e)}")
            await context.abort(grpc.StatusCode.INTERNAL, f"Internal error: {str(e)}")

    @staticmethod
    @auth_required("rec")
    async def rec(user_id: int, context: grpc.ServicerContext = None):
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç gRPC (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –°–ø–∏—Å–æ–∫ ID —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        """
        if context:
            await RecService._handle_request(context)
            
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏–∑ –∫–µ—à–∞
            cache = RecommendationCache()
            cached_recommendations = await cache.get_user_recommendations(user_id)
            
            if cached_recommendations:
                logger.debug(f"–ù–∞–π–¥–µ–Ω—ã –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
                return cached_recommendations
                
            # –ï—Å–ª–∏ –∫–µ—à –ø—É—Å—Ç, –≤—ã—á–∏—Å–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            logger.debug(f"–ö–µ—à –ø—É—Å—Ç –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}, –≤—ã—á–∏—Å–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
            model, dataset = await ModelManager().get_model()
            
            recos = model.recommend(users=[user_id], dataset=dataset, k=40, filter_viewed=True)
            recommendations = recos['item_id'].tolist()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –≤ –∫–µ—à
            await cache.cache_user_recommendations(user_id, recommendations)
            
            return recommendations
            
        except Exception as e:
            error_msg = f"üö® Recommendation error: {str(e)}"
            logger.error(error_msg)
            if context:
                await context.abort(grpc.StatusCode.INTERNAL, error_msg)
            raise

    @classmethod
    async def _scheduled_train(cls):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        try:
            logger.info("‚è≥ Starting scheduled training...")
            await ModelManager().train()
            logger.info("‚úÖ Scheduled training completed successfully")
        except Exception as e:
            logger.error(f"üö® Scheduled training failed: {str(e)}")
    
    @classmethod
    async def _check_scheduled_jobs(cls):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–¥–∞–Ω–∏—è –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ"""
        try:
            registry = ModelRegistry()
            pending_jobs = registry.get_pending_jobs()
            
            for job in pending_jobs:
                job_id = job['id']
                parameters = job['parameters']
                
                try:
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞–Ω–∏—è
                    registry.update_training_job(
                        job_id=job_id,
                        status='running',
                        started_at=datetime.datetime.now()
                    )
                    
                    # –í—ã–ø–æ–ª–Ω—è–µ–º –æ–±—É—á–µ–Ω–∏–µ
                    model_id = await ModelManager().train(parameters)
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞–Ω–∏—è
                    registry.update_training_job(
                        job_id=job_id,
                        status='completed',
                        model_id=model_id,
                        completed_at=datetime.datetime.now()
                    )
                    
                    logger.info(f"‚úÖ Scheduled job {job_id} completed successfully, model ID: {model_id}")
                    
                except Exception as e:
                    # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
                    registry.update_training_job(
                        job_id=job_id,
                        status='failed',
                        completed_at=datetime.datetime.now()
                    )
                    
                    logger.error(f"üö® Scheduled job {job_id} failed: {str(e)}")
                    
        except Exception as e:
            logger.error(f"üö® Error checking scheduled jobs: {str(e)}")

    @staticmethod
    @auth_required("relevant")
    async def relevant(item_id: int, context: grpc.ServicerContext = None):
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–æ—Ö–æ–∂–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        
        Args:
            item_id: ID —ç–ª–µ–º–µ–Ω—Ç–∞
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç gRPC (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –°–ø–∏—Å–æ–∫ ID –ø–æ—Ö–æ–∂–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        """
        if context:
            await RecService._handle_request(context)
            
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏–∑ –∫–µ—à–∞
            cache = RecommendationCache()
            cached_recommendations = await cache.get_item_recommendations(item_id)
            
            if cached_recommendations:
                logger.debug(f"–ù–∞–π–¥–µ–Ω—ã –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —ç–ª–µ–º–µ–Ω—Ç–∞ {item_id}")
                return cached_recommendations
                
            # –ï—Å–ª–∏ –∫–µ—à –ø—É—Å—Ç, –≤—ã—á–∏—Å–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            logger.debug(f"–ö–µ—à –ø—É—Å—Ç –¥–ª—è —ç–ª–µ–º–µ–Ω—Ç–∞ {item_id}, –≤—ã—á–∏—Å–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
            model, dataset = await ModelManager().get_model()
            
            recos = model.recommend_to_items(
                target_items=[item_id],
                dataset=dataset,
                k=40,
                filter_itself=True,
            )
            recommendations = recos['item_id'].tolist()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –≤ –∫–µ—à
            await cache.cache_item_recommendations(item_id, recommendations)
            
            return recommendations
            
        except Exception as e:
            error_msg = f"üö® Recommendation error: {str(e)}"
            logger.error(error_msg)
            if context:
                await context.abort(grpc.StatusCode.INTERNAL, error_msg)
            raise

    @staticmethod
    @auth_required("train")
    async def train(context: Optional[grpc.ServicerContext] = None):
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        
        Args:
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç gRPC (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –æ–±—É—á–µ–Ω–∏—è
        """
        try:
            model_id = await ModelManager().train()
            
            if context:
                return {"status": "success", "model_id": model_id, "version": ModelManager().current_version}
            return model_id
            
        except Exception as e:
            error_msg = f"üî• Training failed: {str(e)}"
            logger.error(error_msg)
            if context:
                await context.abort(grpc.StatusCode.INTERNAL, error_msg)
            raise
            
    @staticmethod
    @auth_required("get_user_recent_interactions")
    async def get_user_recent_interactions(user_id: int, limit: int = 10, context: grpc.ServicerContext = None):
        """
        –ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç gRPC (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            DataFrame —Å –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        """
        try:
            dp_service = DataPrepareService(session_maker=None)  # –ü–æ–ª—É—á–∞–µ–º –∏–∑ DI
            interactions = await dp_service.get_user_recent_interactions(user_id, limit)
            return interactions
            
        except Exception as e:
            error_msg = f"üö® Error getting user interactions: {str(e)}"
            logger.error(error_msg)
            if context:
                await context.abort(grpc.StatusCode.INTERNAL, error_msg)
            raise
